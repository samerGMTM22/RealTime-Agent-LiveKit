I'll search for official LiveKit documentation and repositories to provide you with an accurate, step-by-step guide based on their patterns and flows.Let me search for the official LiveKit GitHub repository examples for the multimodal agent implementation.Let me fetch the actual implementation code from the official LiveKit multimodal agent example.Let me search for the raw content of the multimodal agent Python implementation.Let me search for the specific OpenAI Realtime API example from LiveKit's official examples.Based on the official LiveKit documentation and repositories, here's a comprehensive step-by-step guide for building a LiveKit voice agent using the OpenAI Realtime API:

## Step-by-Step Guide: Building a LiveKit Voice Agent with OpenAI Realtime API

### Phase 1: Prerequisites and Setup

#### 1.1 System Requirements
The MultimodalAgent class in the LiveKit Agents Framework uses the OpenAI Realtime API for speech-to-speech interactions between AI voice assistants and end users. It is implemented in both our Python and Node Agents Framework libraries.

- Python 3.11 or higher
- OpenAI API key with **Realtime API access** (requires special approval)
- LiveKit Cloud account or self-hosted LiveKit server
- Virtual environment for Python dependencies

#### 1.2 Install LiveKit CLI
The following steps take you through the process of creating a LiveKit account and using the LiveKit CLI to create an agent from some minimal templates.

```bash
# Install LiveKit CLI
brew install livekit-cli/tap/lk
# or download from https://github.com/livekit/livekit-cli
```

### Phase 2: Create Agent Project

#### 2.1 Bootstrap Agent from Template
Clone a starter template for your preferred language using the CLI: Enter your OpenAI API Key when prompted.

```bash
# Create a new multimodal agent project
lk app create --template multimodal-agent-python my-realtime-agent
cd my-realtime-agent
```

#### 2.2 Set Up Virtual Environment
```bash
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

#### 2.3 Install Dependencies
To install the core Agents library, along with plugins for popular model providers: pip install "livekit-agents[openai,silero,deepgram,cartesia,turn-detector]~=1.0"

```bash
pip install "livekit-agents[openai]~=1.0"
pip install python-dotenv
```

### Phase 3: Configure Environment

#### 3.1 Create Environment File
Create a `.env.local` file with your credentials:

```bash
# LiveKit server configuration
LIVEKIT_URL=wss://your-project.livekit.cloud
LIVEKIT_API_KEY=your-api-key
LIVEKIT_API_SECRET=your-api-secret

# OpenAI configuration
OPENAI_API_KEY=your-openai-api-key
```

#### 3.2 Use LiveKit CLI for Configuration (Alternative)
```bash
lk app env
```

### Phase 4: Implement the Realtime Agent

#### 4.1 Create the Agent File (`agent.py`)

The OpenAI plugin requires an OpenAI API key. Set OPENAI_API_KEY in your .env file. Use the OpenAI Realtime API within an AgentSession.

```python
import logging
import os
from typing import Annotated
from dotenv import load_dotenv

from livekit import rtc
from livekit.agents import (
    AutoSubscribe,
    JobContext,
    WorkerOptions,
    cli,
    llm,
)
from livekit.agents.multimodal import MultimodalAgent
from livekit.plugins import openai

# Load environment variables
load_dotenv()

# Configure logging
logger = logging.getLogger("multimodal-agent")
logger.setLevel(logging.INFO)

async def entrypoint(ctx: JobContext):
    """Main entry point for the LiveKit agent."""
    logger.info(f"Agent started for room: {ctx.room.name}")
    
    # Connect to the room with audio-only subscription
    await ctx.connect(auto_subscribe=AutoSubscribe.AUDIO_ONLY)
    
    # Wait for a participant to join
    participant = await ctx.wait_for_participant()
    logger.info(f"Participant joined: {participant.identity}")
    
    # Initialize the OpenAI Realtime model
    model = openai.realtime.RealtimeModel(
        instructions=(
            "You are a helpful AI assistant. Be friendly, concise, and helpful. "
            "Respond naturally to the user's questions and maintain a conversational tone."
        ),
        voice="coral",  # Options: alloy, echo, fable, onyx, nova, shimmer
        temperature=0.8,
        modalities=["audio", "text"],
        turn_detection=openai.realtime.TurnDetection(
            type="server_vad",
            threshold=0.5,
            prefix_padding_ms=300,
            silence_duration_ms=700
        )
    )
    
    # Create and start the MultimodalAgent
    agent = MultimodalAgent(
        model=model,
        chat_ctx=llm.ChatContext().append(
            role="system",
            text="You are a voice assistant powered by LiveKit and OpenAI's Realtime API."
        )
    )
    
    # Start the agent session
    session = agent.start(ctx.room, participant)
    
    # Generate initial greeting
    session.conversation.item.create(
        llm.ChatMessage(
            role="assistant",
            content="Hello! I'm your AI assistant. How can I help you today?"
        )
    )
    
    # Handle agent events
    @agent.on("user_started_speaking")
    def on_user_started_speaking():
        logger.info("User started speaking")
    
    @agent.on("user_stopped_speaking")
    def on_user_stopped_speaking():
        logger.info("User stopped speaking")
    
    @agent.on("agent_started_speaking")
    def on_agent_started_speaking():
        logger.info("Agent started speaking")
    
    @agent.on("agent_stopped_speaking")
    def on_agent_stopped_speaking():
        logger.info("Agent stopped speaking")

if __name__ == "__main__":
    # Run the agent worker
    cli.run_app(WorkerOptions(entrypoint_fnc=entrypoint))
```

### Phase 5: Advanced Configuration

#### 5.1 Turn Detection Options

Server VAD (default): Uses periods of silence to automatically chunk the audio. Semantic VAD: Uses a semantic classifier to detect when the user has finished speaking based on their words.

```python
# Server VAD configuration (default)
turn_detection = openai.realtime.TurnDetection(
    type="server_vad",
    threshold=0.5,
    prefix_padding_ms=300,
    silence_duration_ms=700,
    create_response=True
)

# Semantic VAD configuration (more natural conversations)
turn_detection = openai.realtime.TurnDetection(
    type="semantic_vad"
)
```

#### 5.2 Add Function Calling

```python
from livekit.agents import llm, RunContext
from livekit.agents.llm import FunctionContext, function_tool

@function_tool
async def get_weather(
    location: Annotated[str, "The location to get weather for"]
) -> str:
    """Get the current weather for a location."""
    # Implement your weather API call here
    return f"The weather in {location} is sunny and 72Â°F"

# In your entrypoint function:
fnc_ctx = FunctionContext()
fnc_ctx.add_function(get_weather)

agent = MultimodalAgent(
    model=model,
    fnc_ctx=fnc_ctx
)
```

### Phase 6: Running the Agent

#### 6.1 Development Mode
```bash
python agent.py dev
```

#### 6.2 Production Mode
```bash
python agent.py start
```

#### 6.3 Worker Options

Worker: The main process that coordinates job scheduling and launches agents for user sessions.

```python
# Advanced worker configuration
worker_options = WorkerOptions(
    entrypoint_fnc=entrypoint,
    num_idle_processes=2,  # Number of pre-warmed processes
    max_workers=10,        # Maximum concurrent agents
    shutdown_process_after_idle_time=60.0  # Cleanup idle processes
)
```

### Phase 7: Testing with Frontend

#### 7.1 Using LiveKit Playground
This agent requires a frontend application to communicate with.

1. Go to [LiveKit Cloud Playground](https://cloud.livekit.io/playground)
2. Connect to your project
3. Start a conversation with your agent

#### 7.2 Create Custom Frontend
```bash
# Bootstrap a frontend template
lk app create --template voice-assistant-frontend my-frontend
```

### Phase 8: Error Handling and Debugging

#### 8.1 Common Issues and Solutions

**Connection Timeout Error:**
```python
# If you see: "generate_reply timed out"
# This usually means:
# 1. No Realtime API access on your OpenAI account
# 2. Invalid API key
# 3. Network/firewall issues

# Add verbose logging:
import logging
logging.basicConfig(level=logging.DEBUG)
```

**No Audio Response:**
```python
# Ensure audio track subscription
@ctx.room.on("track_published")
def on_track_published(publication: rtc.RemoteTrackPublication, participant: rtc.RemoteParticipant):
    if publication.kind == rtc.TrackKind.AUDIO:
        publication.set_subscribed(True)
```

#### 8.2 Monitoring and Metrics

@self._session.on("metrics_collected") def _metrics_collected(metrics: MultimodalLLMMetrics): self.emit("metrics_collected", metrics)

```python
@agent.on("metrics_collected")
def on_metrics_collected(metrics):
    logger.info(f"Latency: {metrics.ttfb}ms")
    logger.info(f"Token usage: {metrics.total_tokens}")
```

### Phase 9: Production Deployment

#### 9.1 Docker Configuration
```dockerfile
FROM python:3.11-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .
CMD ["python", "agent.py", "start"]
```

#### 9.2 Kubernetes Deployment
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: realtime-agent
spec:
  replicas: 3
  template:
    spec:
      containers:
      - name: agent
        image: your-registry/realtime-agent:latest
        env:
        - name: OPENAI_API_KEY
          valueFrom:
            secretKeyRef:
              name: openai-secret
              key: api-key
```

### Phase 10: Best Practices

The framework includes the MultimodalAgent class for building speech-to-speech agents that use the OpenAI Realtime API.

1. **Error Recovery**: Implement reconnection logic for network failures
2. **Resource Management**: Clean up resources in shutdown handlers
3. **Monitoring**: Use LiveKit's built-in metrics and add custom telemetry
4. **Security**: Never expose API keys in code or logs
5. **Testing**: Create unit tests for function tools and integration tests for the full flow

### Important Notes

OpenAI's Realtime API is a WebSocket interface for low-latency audio streaming, best suited for server-to-server use rather than direct consumption by end-user devices.

- The OpenAI Realtime API requires special access (Tier 5 or explicit approval)
- LiveKit handles the WebRTC complexity while your agent focuses on AI logic
- The MultimodalAgent abstracts away the raw WebSocket protocol
- Always test with the latest versions of LiveKit packages

This guide follows the official LiveKit patterns and best practices for building production-ready voice agents with the OpenAI Realtime API.